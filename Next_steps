############
packages
	*) langdetect
	*) Rouge score
	*) bert score
	*) clone bleu score repo
    *) tensor2tensor
###################################################### 
En_tam Machine translation paper:-  http://www.iitp.ac.in/~sukanta.pcs15/pubs/WAT_2018_paper_12.pdf 
Why all these ? :- http://www.unesco.org/new/fileadmin/MULTIMEDIA/HQ/CLT/pdf/FlyerEndangeredLanguages-WebVersion.pdf
######################################
*) finalize the model, find the best model on the test set:- 213 or 232, finetune on beam_search, topk-topp
*) train the bertified_transformer using 	
*) code walk through :- remove extra comments, name tensors, 
						remove duplicate operations
*) Note on how to use the script



Post-training improvements:-
    #) Hyper parameter tuning:- Randomsearch using keras-tuner https://keras-team.github.io/keras-tuner/tutorials/subclass-tuner/
    #) avg checkpoints

Things to watchout for when coding
########################################################
#) meaning full names
#) separate script for specific functions
#) functions should be small
#) one method that clearly say what it does
#) remove composite switch statements
#) identify memory leakage
#  one line space after function name and before return
#) check duplicate code
#) separate script for python ops
#) test cases on inference decoders
#################################
Expected improvements:-
	*) GPU profiler
	*) Make the model predict the output_seq_len
	*) Data augumentation https://colab.research.google.com/drive/1RGWrQv3e0CRDPDROQ3ZmUWTmlRljasGi?authuser=0#scrollTo=E9RYnn9VDE4N
	*) Tensorflow graph optimizations
    *) Tensorflow implementation of BERT score
    *) remove tamil blacklist from the preprocess code
    *) Collect all indian languages from that link
##################
Additional enhancements
	a) Scheduled sampling :- toss a coin and perform teacher forcing if it is a heads else perform autoregressive training 

