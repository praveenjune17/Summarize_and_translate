###################################################### 
En_tam Machine translation paper:-  http://www.iitp.ac.in/~sukanta.pcs15/pubs/WAT_2018_paper_12.pdf 
######################################

*) cast issues with bertified-arch, remove commented code
b) load vocab size automatically


Machine translation vs summarization:-
	a) change inference decoder hyper parameters for summarization
	b)

Post-training improvements:-
    #) Hyper parameter tuning on beam size, length penalty, dropout, label smoothing
	#) Multiple occurences of stop values in beam search and stop repeated occurences of words
	#) Add (follow the same preprocessing in the tfds pipeline) in the client script and also remove vulgar words, add spell correction, lower casing 

Work on:-
	#) reduce size of monitor
	#) test bertified arch and summarize
	#) Add bertified refine decoder
	#) Client script
	#) run unit test for the last architecture


Things to watchout for when coding
########################################################
#) meaning full names
#) separate script for specific functions
#) functions should be small
#) one method that clearly say what it does
#) remove composite switch statements
#) identify memory leakage
#  one line space after function name and before return
#) check duplicate code
#) separate script for python ops
#) test cases on inference decoders
#################################
Expected improvements:-
	*) GPU profiler
	*) Make the model predict the output_seq_len
	*) Data augumentation https://colab.research.google.com/drive/1RGWrQv3e0CRDPDROQ3ZmUWTmlRljasGi?authuser=0#scrollTo=E9RYnn9VDE4N
	*) Tensorflow graph optimizations
    *) Tensorflow implementation of BERT score
##################
Additional enhancements
	a) Scheduled sampling :- perform teacher forcing or autoregressive training for a coin toss

