Why ? :- http://www.unesco.org/new/fileadmin/MULTIMEDIA/HQ/CLT/pdf/FlyerEndangeredLanguages-WebVersion.pdf
###################################################
Packages in colab + Additonal packages
	*) langdetect
	*) Rouge score
	*) bert score
	*) clone bleu score repo (models/official/google)
    *) tensor2tensor
###################################################
Papers:-
	*) Probing BERT and GPT
	*) Electra
	*) Energy based models for text:- https://arxiv.org/pdf/2004.10188.pdf

#####################################################
Addtional refine mechanism:-
		a) Electra style
		b) Conditional Masked Language Modeling (C-MLM) :- make the generated output coherent by establishing a global context before 
				generating tokens 
		c) mixmatch
		e) policy gradient approach (start with)
			a) immediate reward:- autoregressive score using decoder.. like beam search
			b) BERT score :- reward to use once the sentence is generated 
###################################################
Architectural refinements
	a) vocab sharing with in input and target languages
	b) Reformer :- optimized transformer architecture to better support long sequence length 
	c) Replacing self-attention by CNN.. unified architecture for NLP and vision. hugging face's papers
	d) Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies https://arxiv.org/pdf/1905.05475.pdf
###################################################
Investigate:-
	*) start training a nn when its loss is 0..illaya's paper
			double descent:- happens after early stopping
	*) does gradient accumulation gets skipped when intefered by validation steps?
###################################################
Learning

	*) policy gradients
	*) RNN attention https://www.youtube.com/watch?v=XXtpJxZBa2c&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=8
###################################################
Bugs

	*) BERT score calculation
	*) weighted_and_unified_metric computation
		BLEU 5.118620,
        BERT_f1 61.050230
        op:- 0.0277
###################################################
Coding

#) save model during keyboard interrupt
#) automatically suggest tokens per batch based on the available GPU memory
#) source_text == decode(encode(source_text))
#) modify the tokenization code assuming that all tokenizers are created using hugging face tokenizers
#) functions should be small and should be specific
#) a script should say what it does
#) avoid composite switch statements
#) remove unused python packages
#) one line space after function name and before return
#) check duplicate code
###################################################
Expected improvements

	*) Add electra multilingual pretrained model when they are out	
	*) Make the model predict the output_seq_len
	*) Data augumentation https://colab.research.google.com/drive/1RGWrQv3e0CRDPDROQ3ZmUWTmlRljasGi?authuser=0#scrollTo=E9RYnn9VDE4N
    *) Remove Tamil blacklist from the preprocess code
    *) Collect all indian languages from that link
###################################################
Additional enhancements

	#) Tensorflow profiler
	#) Tensorflow graph optimizations
	#) Hyper parameter tuning using keras tuner
		Randomsearch using keras-tuner https://keras-team.github.io/keras-tuner/tutorials/subclass-tuner/
    #) Avg checkpoints
	#) Support distributed training, 
		*)remove tf.py_function
		*)ship the dataset to a bucket
    #) Tensorflow implementation of BERT score
    #) Scheduled sampling :- toss a coin and perform teacher forcing if it is a heads else perform autoregressive training 
	