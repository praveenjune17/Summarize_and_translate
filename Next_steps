############
packages
	*) langdetect
	*) Rouge score
	*) bert score
	*) clone bleu score repo

###################################################### 
En_tam Machine translation paper:-  http://www.iitp.ac.in/~sukanta.pcs15/pubs/WAT_2018_paper_12.pdf 
######################################
*) code walk through :- reduce size of monitor, remove extra comments, name tensors, remove duplicate operations
*) test summarize

Post-training improvements:-
    #) Hyper parameter tuning on beam size, length penalty, dropout, label smoothing
	#) Multiple occurences of stop values in beam search and stop repeated occurences of words

Work on:-
	
	#) Client script

Things to watchout for when coding
########################################################
#) meaning full names
#) separate script for specific functions
#) functions should be small
#) one method that clearly say what it does
#) remove composite switch statements
#) identify memory leakage
#  one line space after function name and before return
#) check duplicate code
#) separate script for python ops
#) test cases on inference decoders
#################################
Expected improvements:-
	*) GPU profiler
	*) Make the model predict the output_seq_len
	*) Data augumentation https://colab.research.google.com/drive/1RGWrQv3e0CRDPDROQ3ZmUWTmlRljasGi?authuser=0#scrollTo=E9RYnn9VDE4N
	*) Tensorflow graph optimizations
    *) Tensorflow implementation of BERT score
##################
Additional enhancements
	a) Scheduled sampling :- toss a coin and perform teacher forcing if it is a heads else perform autoregressive training 

